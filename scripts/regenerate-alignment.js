#!/usr/bin/env node
/**
 * regenerate-alignment.js — Fix alignment data using meaning-based matching
 *
 * The original alignment was generated by Haiku assuming sequential phrase order,
 * but Shoghi Effendi frequently rearranges clause order in translation. This script
 * re-generates alignment using Sonnet with a prompt that:
 *   1. Breaks source text into sequential phrases
 *   2. Maps each to the corresponding English, wherever it appears in translation
 *   3. Requires exact substrings (no paraphrasing)
 *   4. Allows unmatched phrases when no clear correspondence exists
 *
 * Usage:
 *   ANTHROPIC_API_KEY=sk-... node scripts/regenerate-alignment.js
 *   ANTHROPIC_API_KEY=sk-... node scripts/regenerate-alignment.js --work will-and-testament
 *   ANTHROPIC_API_KEY=sk-... node scripts/regenerate-alignment.js --work will-and-testament --para 1
 *   ANTHROPIC_API_KEY=sk-... node scripts/regenerate-alignment.js --dry-run
 *   ANTHROPIC_API_KEY=sk-... node scripts/regenerate-alignment.js --concurrency 5
 */
import fs from 'node:fs';
import path from 'node:path';
import Anthropic from '@anthropic-ai/sdk';

// Load .env
const CWD = process.cwd();
const envPath = path.join(CWD, '.env');
if (fs.existsSync(envPath)) {
  for (const line of fs.readFileSync(envPath, 'utf-8').split('\n')) {
    const match = line.match(/^(\w+)=["']?(.+?)["']?\s*$/);
    if (match && !process.env[match[1]]) process.env[match[1]] = match[2];
  }
}

const CORPUS_DIR = path.join(CWD, 'src/content/corpus');
const PUBLIC_DIR = path.join(CWD, 'public/_corpus');
const args = process.argv.slice(2);
const DRY_RUN = args.includes('--dry-run');
const workFlag = args.indexOf('--work');
const paraFlag = args.indexOf('--para');
const concFlag = args.indexOf('--concurrency');
const WORK_FILTER = workFlag !== -1 ? args[workFlag + 1] : null;
const PARA_FILTER = paraFlag !== -1 ? parseInt(args[paraFlag + 1]) : null;
const CONCURRENCY = concFlag !== -1 ? parseInt(args[concFlag + 1]) : 3;

const apiKey = process.env.ANTHROPIC_API_KEY;
if (!apiKey && !DRY_RUN) {
  console.error('Set ANTHROPIC_API_KEY environment variable (or use --dry-run)');
  process.exit(1);
}
const client = !DRY_RUN ? new Anthropic({ apiKey }) : null;

// ── Index-based prompt ──
// We present both texts as indexed word lists. The AI returns index-range
// mappings instead of text, so diacritic mismatches are impossible.

function splitWords(text) {
  const words = [];
  const re = /\S+/g;
  let m;
  while ((m = re.exec(text)) !== null) {
    words.push({ word: m[0], start: m.index, end: m.index + m[0].length });
  }
  return words;
}

function buildPrompt(sourceText, translation, sourceLang) {
  const srcWords = splitWords(sourceText);
  const enWords = splitWords(translation);

  const srcList = srcWords.map((w, i) => `[${i}] ${w.word}`).join('\n');
  const enList = enWords.map((w, i) => `[${i}] ${w.word}`).join('\n');

  return `You are a specialist in Arabic, Persian, and Baha'i sacred texts translated by Shoghi Effendi.

TASK: Create a word-level alignment between the indexed source words and indexed English words below. This is a study aid — readers hover over a source word to see which English word(s) it maps to.

SOURCE WORDS (${sourceLang === 'ar' ? 'Arabic' : 'Persian'}):
${srcList}

ENGLISH WORDS:
${enList}

APPROACH — work in two passes:

PASS 1 (phrases): Identify multi-word phrase correspondences first (إضافة constructs, verb+object, noun+adjective). Map source index ranges to English index ranges. The English may appear ANYWHERE — Shoghi Effendi often rearranges clause order.

PASS 2 (remaining words): Go back through BOTH word lists and find every word NOT covered in Pass 1. Map each remaining source word to its English counterpart, and vice versa. After this pass, nearly every word on both sides should be covered.

RULES:
1. COMPREHENSIVE coverage. Every word with a counterpart on the other side must be mapped. The only words left unmapped should be those with genuinely no match.
2. Use inclusive index ranges: {"src": [start, end], "en": [start, end]}. For a single word, start === end.
3. Include particles, conjunctions, prepositions, pronouns when they have clear matches.
4. Each index should appear in at most ONE mapping (no overlaps).
5. Do NOT force wrong matches. But DO include every correct match.

Return ONLY a JSON array: [{"src": [0, 0], "en": [0, 0]}, {"src": [3, 4], "en": [5, 8]}, ...]
No markdown fences, no commentary.`;
}

// ── Convert index-based mappings to {ar, en} text pairs ──

function indicesToPairs(mappings, sourceText, translation) {
  const srcWords = splitWords(sourceText);
  const enWords = splitWords(translation);
  const pairs = [];

  for (const m of mappings) {
    if (!m.src || !m.en || !Array.isArray(m.src) || !Array.isArray(m.en)) continue;
    const [ss, se] = m.src;
    const [es, ee] = m.en;
    if (typeof ss !== 'number' || typeof se !== 'number' || typeof es !== 'number' || typeof ee !== 'number') continue;
    if (ss < 0 || se >= srcWords.length || es < 0 || ee >= enWords.length) continue;
    if (ss > se || es > ee) continue;
    if (!srcWords[ss] || !srcWords[se] || !enWords[es] || !enWords[ee]) continue;

    // Extract exact substrings from original texts using character positions
    const arText = sourceText.slice(srcWords[ss].start, srcWords[se].end);
    let enText = translation.slice(enWords[es].start, enWords[ee].end);
    // Strip trailing punctuation for cleaner highlighting
    enText = enText.replace(/[.,;:!?"']+$/, '');

    if (arText && enText) pairs.push({ ar: arText, en: enText });
  }
  return pairs;
}

// ── API call with retry ──

async function callAPI(sourceText, translation, sourceLang) {
  const prompt = buildPrompt(sourceText, translation, sourceLang);
  for (let attempt = 0; attempt < 3; attempt++) {
    try {
      const response = await client.messages.create({
        model: 'claude-sonnet-4-5-20250929',
        max_tokens: 8192,
        messages: [{ role: 'user', content: prompt }],
      });
      const text = response.content[0].text.trim();
      const cleaned = text.replace(/^```(?:json)?\s*/i, '').replace(/\s*```$/i, '');
      const raw = JSON.parse(cleaned);
      // Convert index mappings to text pairs
      const pairs = indicesToPairs(raw, sourceText, translation);
      if (pairs.length === 0) throw new Error('No valid pairs extracted from AI response');
      return pairs;
    } catch (err) {
      if (attempt < 2) {
        console.warn(`  Retry ${attempt + 1}/3: ${err.message}`);
        await new Promise(r => setTimeout(r, 2000 * (attempt + 1)));
      } else {
        throw err;
      }
    }
  }
}

// ── Fuzzy substring matching (snap AI output to exact substrings) ──

const norm = s => s.normalize('NFKD')
  .replace(/[\u064B-\u065F\u0670\u06D6-\u06ED]/g, '') // Arabic tashkil
  .replace(/[\u0300-\u036F]/g, '') // combining diacritics
  .replace(/\s+/g, ' ').trim();

function mapNormPosToOriginal(original, normalized, normStart, normLen) {
  let oi = 0, ni = 0;
  while (ni < normStart && oi < original.length) {
    const nc = original[oi].normalize('NFKD')
      .replace(/[\u064B-\u065F\u0670\u06D6-\u06ED]/g, '')
      .replace(/[\u0300-\u036F]/g, '');
    ni += nc.length;
    oi++;
  }
  while (oi < original.length && /\s/.test(original[oi])) oi++;
  const origStart = oi;
  let consumed = 0;
  while (consumed < normLen && oi < original.length) {
    const nc = original[oi].normalize('NFKD')
      .replace(/[\u064B-\u065F\u0670\u06D6-\u06ED]/g, '')
      .replace(/[\u0300-\u036F]/g, '');
    consumed += nc.length;
    oi++;
  }
  if (origStart >= 0 && oi > origStart) {
    const text = original.slice(origStart, oi).trim();
    return text;
  }
  return null;
}

function fuzzySnap(needle, haystack) {
  if (!needle || !haystack) return null;
  if (haystack.includes(needle)) return needle;
  const normNeedle = norm(needle);
  const normHay = norm(haystack);
  const normPos = normHay.indexOf(normNeedle);
  if (normPos !== -1) {
    return mapNormPosToOriginal(haystack, normHay, normPos, normNeedle.length);
  }
  return null;
}

// ── Validation ──

function validate(alignment, sourceText, translation) {
  const errors = [];
  const usedEn = new Set();

  for (let i = 0; i < alignment.length; i++) {
    const pair = alignment[i];

    if (!pair.ar) {
      errors.push(`pair[${i}]: missing ar`);
      continue;
    }
    if (!sourceText.includes(pair.ar)) {
      errors.push(`pair[${i}]: ar not substring: "${pair.ar.slice(0, 40)}..."`);
    }
    if (pair.en) {
      if (!translation.includes(pair.en)) {
        errors.push(`pair[${i}]: en not substring: "${pair.en.slice(0, 40)}..."`);
      }
      if (usedEn.has(pair.en)) {
        errors.push(`pair[${i}]: duplicate en: "${pair.en.slice(0, 40)}..."`);
      }
      usedEn.add(pair.en);
    }
  }
  return errors;
}

// ── Process one file ──

async function processFile(filePath) {
  const data = JSON.parse(fs.readFileSync(filePath, 'utf8'));
  if (!data.source_text || !data.translation) return { status: 'skip' };

  const newAlignment = await callAPI(data.source_text, data.translation, data.source_lang || 'ar');
  if (!Array.isArray(newAlignment)) return { status: 'bad_response' };

  const errors = validate(newAlignment, data.source_text, data.translation);

  // Pairs are constructed from exact character positions, so they should
  // always be valid substrings. Filter just in case.
  const cleaned = newAlignment.filter((pair, i) => {
    if (!pair.ar || !data.source_text.includes(pair.ar)) {
      if (PARA_FILTER) console.log(`    REJECTED ar[${i}]: "${pair.ar?.slice(0,40)}"`);
      return false;
    }
    if (!pair.en || !data.translation.includes(pair.en)) {
      if (PARA_FILTER) console.log(`    REJECTED en[${i}]: "${pair.en?.slice(0,40)}"`);
      return false;
    }
    return true;
  });

  // Remove duplicate en values (keep first occurrence)
  const seenEn = new Set();
  const deduped = cleaned.filter(pair => {
    if (!pair.en) return true;
    if (seenEn.has(pair.en)) return false;
    seenEn.add(pair.en);
    return true;
  });

  if (!DRY_RUN) {
    data.alignment = deduped;
    fs.writeFileSync(filePath, JSON.stringify(data, null, 2) + '\n');
    // Also update public/_corpus mirror if it exists
    const rel = path.relative(CORPUS_DIR, filePath);
    const pubPath = path.join(PUBLIC_DIR, rel);
    if (fs.existsSync(pubPath)) {
      const pubData = JSON.parse(fs.readFileSync(pubPath, 'utf8'));
      pubData.alignment = deduped;
      fs.writeFileSync(pubPath, JSON.stringify(pubData, null, 2) + '\n');
    }
  }

  return {
    status: 'ok',
    total: newAlignment.length,
    valid: deduped.length,
    errors: errors.length,
    errorDetails: errors.slice(0, 3),
  };
}

// ── Collect files ──

function collectFiles() {
  const files = [];
  const works = WORK_FILTER ? [WORK_FILTER] : fs.readdirSync(CORPUS_DIR);

  for (const work of works) {
    const workDir = path.join(CORPUS_DIR, work);
    if (!fs.existsSync(workDir) || !fs.statSync(workDir).isDirectory()) continue;

    for (const file of fs.readdirSync(workDir)) {
      if (!file.endsWith('.json') || file.startsWith('_')) continue;
      const paraNum = parseInt(file);
      if (PARA_FILTER && paraNum !== PARA_FILTER) continue;
      files.push({ path: path.join(workDir, file), work, para: paraNum });
    }
  }

  files.sort((a, b) => a.work.localeCompare(b.work) || a.para - b.para);
  return files;
}

// ── Main with concurrency control ──

async function main() {
  const files = collectFiles();
  console.log(`\nAlignment regeneration: ${files.length} files, concurrency ${CONCURRENCY}${DRY_RUN ? ' (DRY RUN)' : ''}\n`);

  let processed = 0, failed = 0, skipped = 0, totalErrors = 0;
  const start = Date.now();

  // Process with bounded concurrency
  let idx = 0;
  async function worker() {
    while (idx < files.length) {
      const file = files[idx++];
      const label = `${file.work} §${file.para}`;
      try {
        const result = await processFile(file.path);
        if (result.status === 'skip') {
          skipped++;
        } else if (result.status === 'ok') {
          processed++;
          totalErrors += result.errors;
          const mark = result.errors > 0 ? `⚠ ${result.errors} errors` : '✓';
          console.log(`  ${label}: ${result.valid}/${result.total} pairs ${mark}`);
        } else {
          failed++;
          console.error(`  ${label}: ${result.status}`);
        }
      } catch (err) {
        failed++;
        console.error(`  ${label}: FAILED — ${err.message}`);
      }
    }
  }

  const workers = Array.from({ length: CONCURRENCY }, () => worker());
  await Promise.all(workers);

  const elapsed = ((Date.now() - start) / 1000).toFixed(1);
  console.log(`\n=== Alignment Regeneration Report ===`);
  console.log(`Processed:   ${processed}`);
  console.log(`Skipped:     ${skipped}`);
  console.log(`Failed:      ${failed}`);
  console.log(`Validation:  ${totalErrors} total errors (filtered out)`);
  console.log(`Time:        ${elapsed}s`);
  console.log(`Mode:        ${DRY_RUN ? 'DRY RUN' : 'APPLIED'}`);
}

main().catch(err => {
  console.error('Fatal:', err);
  process.exit(1);
});

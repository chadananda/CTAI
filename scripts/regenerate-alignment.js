#!/usr/bin/env node
/**
 * regenerate-alignment.js — Fix alignment data using meaning-based matching
 *
 * The original alignment was generated by Haiku assuming sequential phrase order,
 * but Shoghi Effendi frequently rearranges clause order in translation. This script
 * re-generates alignment using Sonnet with a prompt that:
 *   1. Breaks source text into sequential phrases
 *   2. Maps each to the corresponding English, wherever it appears in translation
 *   3. Requires exact substrings (no paraphrasing)
 *   4. Allows unmatched phrases when no clear correspondence exists
 *
 * Usage:
 *   ANTHROPIC_API_KEY=sk-... node scripts/regenerate-alignment.js
 *   ANTHROPIC_API_KEY=sk-... node scripts/regenerate-alignment.js --work will-and-testament
 *   ANTHROPIC_API_KEY=sk-... node scripts/regenerate-alignment.js --work will-and-testament --para 1
 *   ANTHROPIC_API_KEY=sk-... node scripts/regenerate-alignment.js --dry-run
 *   ANTHROPIC_API_KEY=sk-... node scripts/regenerate-alignment.js --concurrency 5
 */
import fs from 'node:fs';
import path from 'node:path';
import Anthropic from '@anthropic-ai/sdk';

// Load .env
const CWD = process.cwd();
const envPath = path.join(CWD, '.env');
if (fs.existsSync(envPath)) {
  for (const line of fs.readFileSync(envPath, 'utf-8').split('\n')) {
    const match = line.match(/^(\w+)=["']?(.+?)["']?\s*$/);
    if (match && !process.env[match[1]]) process.env[match[1]] = match[2];
  }
}

const CORPUS_DIR = path.join(CWD, 'src/content/corpus');
const PUBLIC_DIR = path.join(CWD, 'public/_corpus');
const args = process.argv.slice(2);
const DRY_RUN = args.includes('--dry-run');
const workFlag = args.indexOf('--work');
const paraFlag = args.indexOf('--para');
const concFlag = args.indexOf('--concurrency');
const WORK_FILTER = workFlag !== -1 ? args[workFlag + 1] : null;
const PARA_FILTER = paraFlag !== -1 ? parseInt(args[paraFlag + 1]) : null;
const CONCURRENCY = concFlag !== -1 ? parseInt(args[concFlag + 1]) : 3;

const apiKey = process.env.ANTHROPIC_API_KEY;
if (!apiKey && !DRY_RUN) {
  console.error('Set ANTHROPIC_API_KEY environment variable (or use --dry-run)');
  process.exit(1);
}
const client = !DRY_RUN ? new Anthropic({ apiKey }) : null;

// ── Index-based prompt ──
// We present both texts as indexed word lists. The AI returns index-range
// mappings instead of text, so diacritic mismatches are impossible.

function splitWords(text) {
  const words = [];
  const re = /\S+/g;
  let m;
  while ((m = re.exec(text)) !== null) {
    words.push({ word: m[0], start: m.index, end: m.index + m[0].length });
  }
  return words;
}

// ── Prompt builders for each pass ──

function wordList(words) {
  return words.map((w, i) => `[${i}] ${w.word}`).join('\n');
}

function phrasePrompt(srcWords, enWords, sourceLang) {
  return `You are a specialist in Arabic, Persian, and Baha'i sacred texts translated by Shoghi Effendi.

TASK: Link TIGHT GRAMMATICAL PHRASES between the source and translation. This is Pass 1 — single-word links come later.

A phrase is a minimal grammatical unit: إضافة construct (رَبِّ آبَائِكَ → "the Lord of thy fathers"), prepositional phrase (فِي سُبُلِ → "in the paths"), verb+particle (فَتَوَكَّلْ عَلَى → "Rely upon"), possessive (نَغَمَاتِهِ → "His Melody"). Do NOT join independent elements — if there is an "and"/و or a clause boundary, SPLIT into separate phrases.

CONTEXT: These are Baha'i sacred texts. The speaker is God/the Manifestation using the royal "We" — first person plural verbs like أَشْهَدْنَاهُمْ map to "have We found them" (not third person). Shoghi Effendi uses archaic English (thou, thy, dost, hath).

EXAMPLE: For "اللهِ رَبِّكَ وَرَبِّ آبَائِكَ" / "God, thy God and the Lord of thy fathers":
  - اللهِ → "God" (single word)
  - رَبِّكَ → "thy God" (possessive suffix كَ makes this independent)
  - وَرَبِّ آبَائِكَ → "and the Lord of thy fathers" (إضافة: رَبِّ governs آبَائِكَ, و prefix = "and")
  WRONG: merging رَبِّكَ with وَرَبِّ — these are two separate nouns joined by و.

SOURCE WORDS (${sourceLang === 'ar' ? 'Arabic' : 'Persian'}):
${wordList(srcWords)}

ENGLISH WORDS:
${wordList(enWords)}

RULES:
1. Each phrase must be a SINGLE grammatical unit. Max 2-3 words per side. Never join clauses or items separated by "and"/و.
2. Use inclusive index ranges: {"src": [start, end], "en": [start, end]}.
3. ZERO OVERLAP: each word index may appear in at most ONE link.
4. Handle إضافة correctly: the governing noun takes the preposition "of" (e.g., رَبِّ → "the Lord of", not just "Lord").
5. English may appear ANYWHERE — Shoghi Effendi rearranges clause order.
6. Do NOT force wrong matches. Leave uncertain words for the next pass.
7. When a source word maps to a single English word, leave it for the word pass — only link it here if it MUST be grouped with an adjacent word.

Return ONLY a JSON array: [{"src": [3, 4], "en": [5, 8]}, ...]
No markdown fences, no commentary.`;
}

function wordPrompt(srcWords, enWords, takenSrc, takenEn, sourceLang) {
  // Show which indices are available vs taken
  const srcList = srcWords.map((w, i) => {
    return takenSrc.has(i) ? `[${i}] ${w.word} (TAKEN)` : `[${i}] ${w.word}`;
  }).join('\n');
  const enList = enWords.map((w, i) => {
    return takenEn.has(i) ? `[${i}] ${w.word} (TAKEN)` : `[${i}] ${w.word}`;
  }).join('\n');

  return `You are a specialist in Arabic, Persian, and Baha'i sacred texts translated by Shoghi Effendi.

TASK: Link INDIVIDUAL WORDS that were not linked in the previous phrase-linking pass. Only link words marked as available (not TAKEN). This is a study aid — readers hover a source word to see which specific English word Shoghi Effendi used for it.

SOURCE WORDS (${sourceLang === 'ar' ? 'Arabic' : 'Persian'}):
${srcList}

ENGLISH WORDS:
${enList}

RULES:
1. ONLY link words that are NOT marked (TAKEN). Never reference a TAKEN index.
2. Link single source words to single English words: {"src": [i, i], "en": [j, j]}.
3. You may link a single source word to 2 English words if needed, or vice versa, but prefer 1-to-1.
4. ZERO OVERLAP with each other and with TAKEN indices.
5. Be SPECIFIC: if a word appears multiple times, link to the EXACT occurrence that corresponds to this source word based on meaning and sentence position.
6. Include particles/prepositions/pronouns when they have clear matches.
7. Do NOT force wrong matches. Only link words you are confident about.

Return ONLY a JSON array: [{"src": [0, 0], "en": [2, 2]}, ...]
No markdown fences, no commentary.`;
}

// ── Convert index-based mappings to {ar, en} text pairs ──

function indicesToPairs(mappings, sourceText, translation, srcWords, enWords) {
  const pairs = [];
  for (const m of mappings) {
    if (!m.src || !m.en || !Array.isArray(m.src) || !Array.isArray(m.en)) continue;
    const [ss, se] = m.src;
    const [es, ee] = m.en;
    if (typeof ss !== 'number' || typeof se !== 'number' || typeof es !== 'number' || typeof ee !== 'number') continue;
    if (ss < 0 || se >= srcWords.length || es < 0 || ee >= enWords.length) continue;
    if (ss > se || es > ee) continue;

    const arStart = srcWords[ss].start;
    const arEnd = srcWords[se].end;
    const arText = sourceText.slice(arStart, arEnd);

    const enStart = enWords[es].start;
    let enEnd = enWords[ee].end;
    let enText = translation.slice(enStart, enEnd);
    const stripped = enText.replace(/[.,;:!?"']+$/, '');
    if (stripped) { enEnd = enStart + stripped.length; enText = stripped; }

    if (arText && enText) {
      pairs.push({ ar: arText, en: enText, src: [arStart, arEnd], tgt: [enStart, enEnd], srcIdx: [ss, se], enIdx: [es, ee] });
    }
  }
  return pairs;
}

// ── Strip overlapping pairs (enforce zero overlap on both sides) ──

function removeOverlaps(pairs) {
  const usedSrc = new Set();
  const usedEn = new Set();
  const clean = [];
  for (const p of pairs) {
    const [ss, se] = p.srcIdx;
    const [es, ee] = p.enIdx;
    let overlap = false;
    for (let i = ss; i <= se; i++) { if (usedSrc.has(i)) { overlap = true; break; } }
    if (!overlap) for (let i = es; i <= ee; i++) { if (usedEn.has(i)) { overlap = true; break; } }
    if (overlap) continue;
    for (let i = ss; i <= se; i++) usedSrc.add(i);
    for (let i = es; i <= ee; i++) usedEn.add(i);
    clean.push(p);
  }
  return clean;
}

// ── Single API call with retry ──

async function aiCall(prompt) {
  for (let attempt = 0; attempt < 3; attempt++) {
    try {
      const response = await client.messages.create({
        model: 'claude-sonnet-4-5-20250929',
        max_tokens: 8192,
        messages: [{ role: 'user', content: prompt }],
      });
      const text = response.content[0].text.trim();
      const cleaned = text.replace(/^```(?:json)?\s*/i, '').replace(/\s*```$/i, '');
      return JSON.parse(cleaned);
    } catch (err) {
      if (attempt < 2) {
        await new Promise(r => setTimeout(r, 2000 * (attempt + 1)));
      } else {
        throw err;
      }
    }
  }
}

// ── Multi-pass alignment ──

async function callAPI(sourceText, translation, sourceLang) {
  const srcWords = splitWords(sourceText);
  const enWords = splitWords(translation);

  // Pass 1: phrases (2+ words on at least one side, max 3 per side)
  const p1raw = await aiCall(phrasePrompt(srcWords, enWords, sourceLang));
  const p1pairs = indicesToPairs(Array.isArray(p1raw) ? p1raw : [], sourceText, translation, srcWords, enWords)
    .filter(p => {
      const srcLen = p.srcIdx[1] - p.srcIdx[0] + 1;
      const enLen = p.enIdx[1] - p.enIdx[0] + 1;
      // Must be a phrase (2+ words on at least one side)
      if (srcLen < 2 && enLen < 2) return false;
      // Max 3 source words; English can be longer (Arabic is compact, English wordy)
      if (srcLen > 3 || enLen > 7) return false;
      return true;
    });
  const p1clean = removeOverlaps(p1pairs);

  // Track taken indices
  const takenSrc = new Set();
  const takenEn = new Set();
  for (const p of p1clean) {
    for (let i = p.srcIdx[0]; i <= p.srcIdx[1]; i++) takenSrc.add(i);
    for (let i = p.enIdx[0]; i <= p.enIdx[1]; i++) takenEn.add(i);
  }

  // Pass 2: individual words
  const p2raw = await aiCall(wordPrompt(srcWords, enWords, takenSrc, takenEn, sourceLang));
  const p2pairs = indicesToPairs(Array.isArray(p2raw) ? p2raw : [], sourceText, translation, srcWords, enWords);
  // Filter out any that touch taken indices
  const p2valid = p2pairs.filter(p => {
    for (let i = p.srcIdx[0]; i <= p.srcIdx[1]; i++) { if (takenSrc.has(i)) return false; }
    for (let i = p.enIdx[0]; i <= p.enIdx[1]; i++) { if (takenEn.has(i)) return false; }
    return true;
  });
  const p2clean = removeOverlaps(p2valid);

  const allPairs = [...p1clean, ...p2clean];

  // Strip internal idx fields before returning
  return allPairs.map(({ ar, en, src, tgt }) => ({ ar, en, src, tgt }));
}


// ── Process one file ──

async function processFile(filePath) {
  const data = JSON.parse(fs.readFileSync(filePath, 'utf8'));
  if (!data.source_text || !data.translation) return { status: 'skip' };

  const newAlignment = await callAPI(data.source_text, data.translation, data.source_lang || 'ar');
  if (!Array.isArray(newAlignment)) return { status: 'bad_response' };

  // Pairs are constructed from exact character positions — just sanity check
  const cleaned = newAlignment.filter(pair => pair.ar && pair.en);
  const errors = [];

  // Remove duplicate en values (keep first occurrence)
  const seenEn = new Set();
  const deduped = cleaned.filter(pair => {
    if (!pair.en) return true;
    if (seenEn.has(pair.en)) return false;
    seenEn.add(pair.en);
    return true;
  });

  if (!DRY_RUN) {
    data.alignment = deduped;
    fs.writeFileSync(filePath, JSON.stringify(data, null, 2) + '\n');
    // Also update public/_corpus mirror if it exists
    const rel = path.relative(CORPUS_DIR, filePath);
    const pubPath = path.join(PUBLIC_DIR, rel);
    if (fs.existsSync(pubPath)) {
      const pubData = JSON.parse(fs.readFileSync(pubPath, 'utf8'));
      pubData.alignment = deduped;
      fs.writeFileSync(pubPath, JSON.stringify(pubData, null, 2) + '\n');
    }
  }

  return {
    status: 'ok',
    total: newAlignment.length,
    valid: deduped.length,
    errors: errors.length,
    errorDetails: errors.slice(0, 3),
  };
}

// ── Collect files ──

function collectFiles() {
  const files = [];
  const works = WORK_FILTER ? [WORK_FILTER] : fs.readdirSync(CORPUS_DIR);

  for (const work of works) {
    const workDir = path.join(CORPUS_DIR, work);
    if (!fs.existsSync(workDir) || !fs.statSync(workDir).isDirectory()) continue;

    for (const file of fs.readdirSync(workDir)) {
      if (!file.endsWith('.json') || file.startsWith('_')) continue;
      const paraNum = parseInt(file);
      if (PARA_FILTER && paraNum !== PARA_FILTER) continue;
      files.push({ path: path.join(workDir, file), work, para: paraNum });
    }
  }

  files.sort((a, b) => a.work.localeCompare(b.work) || a.para - b.para);
  return files;
}

// ── Main with concurrency control ──

async function main() {
  const files = collectFiles();
  console.log(`\nAlignment regeneration: ${files.length} files, concurrency ${CONCURRENCY}${DRY_RUN ? ' (DRY RUN)' : ''}\n`);

  let processed = 0, failed = 0, skipped = 0, totalErrors = 0;
  const start = Date.now();

  // Process with bounded concurrency
  let idx = 0;
  async function worker() {
    while (idx < files.length) {
      const file = files[idx++];
      const label = `${file.work} §${file.para}`;
      try {
        const result = await processFile(file.path);
        if (result.status === 'skip') {
          skipped++;
        } else if (result.status === 'ok') {
          processed++;
          totalErrors += result.errors;
          const mark = result.errors > 0 ? `⚠ ${result.errors} errors` : '✓';
          console.log(`  ${label}: ${result.valid}/${result.total} pairs ${mark}`);
        } else {
          failed++;
          console.error(`  ${label}: ${result.status}`);
        }
      } catch (err) {
        failed++;
        console.error(`  ${label}: FAILED — ${err.message}`);
      }
    }
  }

  const workers = Array.from({ length: CONCURRENCY }, () => worker());
  await Promise.all(workers);

  const elapsed = ((Date.now() - start) / 1000).toFixed(1);
  console.log(`\n=== Alignment Regeneration Report ===`);
  console.log(`Processed:   ${processed}`);
  console.log(`Skipped:     ${skipped}`);
  console.log(`Failed:      ${failed}`);
  console.log(`Validation:  ${totalErrors} total errors (filtered out)`);
  console.log(`Time:        ${elapsed}s`);
  console.log(`Mode:        ${DRY_RUN ? 'DRY RUN' : 'APPLIED'}`);
}

main().catch(err => {
  console.error('Fatal:', err);
  process.exit(1);
});

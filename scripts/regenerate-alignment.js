#!/usr/bin/env node
/**
 * regenerate-alignment.js — Fix alignment data using meaning-based matching
 *
 * The original alignment was generated by Haiku assuming sequential phrase order,
 * but Shoghi Effendi frequently rearranges clause order in translation. This script
 * re-generates alignment using Sonnet with a prompt that:
 *   1. Breaks source text into sequential phrases
 *   2. Maps each to the corresponding English, wherever it appears in translation
 *   3. Requires exact substrings (no paraphrasing)
 *   4. Allows unmatched phrases when no clear correspondence exists
 *
 * Usage:
 *   ANTHROPIC_API_KEY=sk-... node scripts/regenerate-alignment.js
 *   ANTHROPIC_API_KEY=sk-... node scripts/regenerate-alignment.js --work will-and-testament
 *   ANTHROPIC_API_KEY=sk-... node scripts/regenerate-alignment.js --work will-and-testament --para 1
 *   ANTHROPIC_API_KEY=sk-... node scripts/regenerate-alignment.js --dry-run
 *   ANTHROPIC_API_KEY=sk-... node scripts/regenerate-alignment.js --concurrency 5
 */
import fs from 'node:fs';
import path from 'node:path';
import Anthropic from '@anthropic-ai/sdk';

// Load .env
const CWD = process.cwd();
const envPath = path.join(CWD, '.env');
if (fs.existsSync(envPath)) {
  for (const line of fs.readFileSync(envPath, 'utf-8').split('\n')) {
    const match = line.match(/^(\w+)=["']?(.+?)["']?\s*$/);
    if (match && !process.env[match[1]]) process.env[match[1]] = match[2];
  }
}

const CORPUS_DIR = path.join(CWD, 'src/content/corpus');
const PUBLIC_DIR = path.join(CWD, 'public/_corpus');
const args = process.argv.slice(2);
const DRY_RUN = args.includes('--dry-run');
const workFlag = args.indexOf('--work');
const paraFlag = args.indexOf('--para');
const concFlag = args.indexOf('--concurrency');
const WORK_FILTER = workFlag !== -1 ? args[workFlag + 1] : null;
const PARA_FILTER = paraFlag !== -1 ? parseInt(args[paraFlag + 1]) : null;
const CONCURRENCY = concFlag !== -1 ? parseInt(args[concFlag + 1]) : 3;

const apiKey = process.env.ANTHROPIC_API_KEY;
if (!apiKey && !DRY_RUN) {
  console.error('Set ANTHROPIC_API_KEY environment variable (or use --dry-run)');
  process.exit(1);
}
const client = !DRY_RUN ? new Anthropic({ apiKey }) : null;

// ── Prompt ──

function buildPrompt(sourceText, translation, sourceLang) {
  return `You are a specialist in Arabic, Persian, and Baha'i sacred texts translated by Shoghi Effendi.

TASK: Create a WORD-LEVEL alignment between the source text and its English translation. This is a study aid — readers hover over an Arabic/Persian word to see which English word(s) Shoghi Effendi used to translate it.

SOURCE TEXT (${sourceLang === 'ar' ? 'Arabic' : 'Persian'}):
${sourceText}

ENGLISH TRANSLATION (by Shoghi Effendi):
${translation}

RULES:
1. Go through the source text word by word, from start to end.
2. For each content word (nouns, verbs, adjectives, adverbs), find the EXACT English word(s) that translate it. The English may appear ANYWHERE in the translation — Shoghi Effendi often rearranges clause order.
3. The "ar" value should be exactly ONE word from the source text (occasionally two if they form an inseparable unit like a proper name or إضافة construct).
4. The "en" value should be the SHORTEST English substring that captures the rendering — typically 1-3 words. Include articles/prepositions only when they are integral to the rendering (e.g. "the Temple" for هیکل, "His Cause" for امره).
5. CRITICAL: Every "ar" value must be a character-for-character exact substring of the source text. Every "en" value must be a character-for-character exact substring of the translation. Copy them exactly — same diacritics, same punctuation, same spacing.
6. Skip particles, conjunctions, and prepositions (و، من، عن، فی، إلی، ب، ل، etc.) unless they carry distinct meaning in the translation.
7. If a source word has no clear English match, omit it entirely.
8. Each English substring should appear in at most ONE pair (no duplicates).
9. Do NOT force matches. A wrong match is far worse than no match.

Return ONLY a JSON array of objects: [{"ar": "...", "en": "..."}, ...]
No markdown fences, no commentary.`;
}

// ── API call with retry ──

async function callAPI(sourceText, translation, sourceLang) {
  const prompt = buildPrompt(sourceText, translation, sourceLang);
  for (let attempt = 0; attempt < 3; attempt++) {
    try {
      const response = await client.messages.create({
        model: 'claude-sonnet-4-5-20250929',
        max_tokens: 8192,
        messages: [{ role: 'user', content: prompt }],
      });
      const text = response.content[0].text.trim();
      const cleaned = text.replace(/^```(?:json)?\s*/i, '').replace(/\s*```$/i, '');
      return JSON.parse(cleaned);
    } catch (err) {
      if (attempt < 2) {
        console.warn(`  Retry ${attempt + 1}/3: ${err.message}`);
        await new Promise(r => setTimeout(r, 2000 * (attempt + 1)));
      } else {
        throw err;
      }
    }
  }
}

// ── Validation ──

function validate(alignment, sourceText, translation) {
  const errors = [];
  const usedEn = new Set();

  for (let i = 0; i < alignment.length; i++) {
    const pair = alignment[i];

    if (!pair.ar) {
      errors.push(`pair[${i}]: missing ar`);
      continue;
    }
    if (!sourceText.includes(pair.ar)) {
      errors.push(`pair[${i}]: ar not substring: "${pair.ar.slice(0, 40)}..."`);
    }
    if (pair.en) {
      if (!translation.includes(pair.en)) {
        errors.push(`pair[${i}]: en not substring: "${pair.en.slice(0, 40)}..."`);
      }
      if (usedEn.has(pair.en)) {
        errors.push(`pair[${i}]: duplicate en: "${pair.en.slice(0, 40)}..."`);
      }
      usedEn.add(pair.en);
    }
  }
  return errors;
}

// ── Process one file ──

async function processFile(filePath) {
  const data = JSON.parse(fs.readFileSync(filePath, 'utf8'));
  if (!data.source_text || !data.translation) return { status: 'skip' };

  const newAlignment = await callAPI(data.source_text, data.translation, data.source_lang || 'ar');
  if (!Array.isArray(newAlignment)) return { status: 'bad_response' };

  const errors = validate(newAlignment, data.source_text, data.translation);

  // Filter out invalid pairs rather than rejecting the whole result
  const cleaned = newAlignment.filter((pair, i) => {
    if (!pair.ar || !data.source_text.includes(pair.ar)) return false;
    if (pair.en && !data.translation.includes(pair.en)) return false;
    return true;
  });

  // Remove duplicate en values (keep first occurrence)
  const seenEn = new Set();
  const deduped = cleaned.filter(pair => {
    if (!pair.en) return true;
    if (seenEn.has(pair.en)) return false;
    seenEn.add(pair.en);
    return true;
  });

  if (!DRY_RUN) {
    data.alignment = deduped;
    fs.writeFileSync(filePath, JSON.stringify(data, null, 2) + '\n');
    // Also update public/_corpus mirror if it exists
    const rel = path.relative(CORPUS_DIR, filePath);
    const pubPath = path.join(PUBLIC_DIR, rel);
    if (fs.existsSync(pubPath)) {
      const pubData = JSON.parse(fs.readFileSync(pubPath, 'utf8'));
      pubData.alignment = deduped;
      fs.writeFileSync(pubPath, JSON.stringify(pubData, null, 2) + '\n');
    }
  }

  return {
    status: 'ok',
    total: newAlignment.length,
    valid: deduped.length,
    errors: errors.length,
    errorDetails: errors.slice(0, 3),
  };
}

// ── Collect files ──

function collectFiles() {
  const files = [];
  const works = WORK_FILTER ? [WORK_FILTER] : fs.readdirSync(CORPUS_DIR);

  for (const work of works) {
    const workDir = path.join(CORPUS_DIR, work);
    if (!fs.existsSync(workDir) || !fs.statSync(workDir).isDirectory()) continue;

    for (const file of fs.readdirSync(workDir)) {
      if (!file.endsWith('.json') || file.startsWith('_')) continue;
      const paraNum = parseInt(file);
      if (PARA_FILTER && paraNum !== PARA_FILTER) continue;
      files.push({ path: path.join(workDir, file), work, para: paraNum });
    }
  }

  files.sort((a, b) => a.work.localeCompare(b.work) || a.para - b.para);
  return files;
}

// ── Main with concurrency control ──

async function main() {
  const files = collectFiles();
  console.log(`\nAlignment regeneration: ${files.length} files, concurrency ${CONCURRENCY}${DRY_RUN ? ' (DRY RUN)' : ''}\n`);

  let processed = 0, failed = 0, skipped = 0, totalErrors = 0;
  const start = Date.now();

  // Process with bounded concurrency
  let idx = 0;
  async function worker() {
    while (idx < files.length) {
      const file = files[idx++];
      const label = `${file.work} §${file.para}`;
      try {
        const result = await processFile(file.path);
        if (result.status === 'skip') {
          skipped++;
        } else if (result.status === 'ok') {
          processed++;
          totalErrors += result.errors;
          const mark = result.errors > 0 ? `⚠ ${result.errors} errors` : '✓';
          console.log(`  ${label}: ${result.valid}/${result.total} pairs ${mark}`);
        } else {
          failed++;
          console.error(`  ${label}: ${result.status}`);
        }
      } catch (err) {
        failed++;
        console.error(`  ${label}: FAILED — ${err.message}`);
      }
    }
  }

  const workers = Array.from({ length: CONCURRENCY }, () => worker());
  await Promise.all(workers);

  const elapsed = ((Date.now() - start) / 1000).toFixed(1);
  console.log(`\n=== Alignment Regeneration Report ===`);
  console.log(`Processed:   ${processed}`);
  console.log(`Skipped:     ${skipped}`);
  console.log(`Failed:      ${failed}`);
  console.log(`Validation:  ${totalErrors} total errors (filtered out)`);
  console.log(`Time:        ${elapsed}s`);
  console.log(`Mode:        ${DRY_RUN ? 'DRY RUN' : 'APPLIED'}`);
}

main().catch(err => {
  console.error('Fatal:', err);
  process.exit(1);
});
